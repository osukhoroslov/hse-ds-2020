# Распределённые блокировки

Сегодня мы поговорим о распределённых алгоритмах в целом и общем, и пока мы не
добрались до распределённого консенсуса, поговорим о немного более лёгких вещах
&mdash; о блокировках. Вхождение в ту или иную критическую секцию бывает очень
важным для приложений, будь то это обновление данных, будь это просто
распределение конкурентной информации (например, при хэджировании запросов).
Блокировки встречаются в системах, возможно не так часто, как распределённые
консенсусы, но тем не менее, о них полезно знать.

# Централизованные подходы

Самым простым подходом в таких системах являются централизованные. Один из
узлов будет считаться главным, и он будет раздавать разрешения на критические
секции. Такой подход ещё иногда преобразовывают в подход с раздачей токенов,
например, мастер генерирует токен, который будет принимать критическая секция
(например, другой сервер или что-то иное), перед тем как зайти в
критическую секцию, она принимает только выданные токены. Токены также могут
быть аннотированны временем, что означает, что прийти с этим токеном можно
только в этот промежуток и тогда может быть узлу дадут доступ.

Иногда также вводят понятия _lease_, оно означает, что узел хочет продлить
свою критическую секцию после выданного ему токена. Для получения lease от
мастера, он может все выданные токены отозвать, сказать об этом критической
секции и выдать разрешение на продление критической секции. Стоит отметить, что
иногда критическая секция и мастер совпадают, тем не менее, их принято различать
как отдельные сервисы, так как они выполняют разную работу.

Понятное дело, что такие подходы не обладают устойчивостью к падению мастера.

# Lamport's Mutex

Если рассматривать распределённые алгоритмы, которые не требуют
централизованного подхода, то одним из самых первых был алгоритм Лэмпорта.

Как мы уже разбирали часы Лэмпорта (с разрешением конфликтов по номеру ноды),
этот алгоритм работает по ним, где время запросов увеличивается только при
запросах критической секции (но при подтверждениях мы только будем запоминать
пришедшие таймстемпы). Также будем предполагать, что никакой узел не задает
запрос на критическую секцию дважды пока не получит от всех ответы. Это нужно
для корректности алгоритма. Сам алгоритм:

Запрос:
* Узел `i` посылает всем запрос `Req(ts, i)` и кладёт в свою очередь этот запрос в порядке `ts`
* Когда узел `j` получает `Req(ts, i)`, он возвращает `Ack` узлу `i` и кладёт этот запрос в себе в очередь (напомним, очередь отсортирована по timestamp от часов Лэмпорта)

Вход в критическую секцию происходит только тогда, когда запрос находится первым
в очереди, и все остальные ответили процессу. Как только критическая секция заканчивается,
процесс посылает всем `Release` сообщение и после этого все убирают сообщение из
начала очереди.

Доказательство корректности достаточно прямолинейное, например, можно предположить,
что два процесса зашли в критическую секцию, значит они должны были обменяться
ответами и запросами, а так как у них очереди отсортированы по времени, то только
один запрос мог оказаться у обоих в главе очереди.

Этот алгоритм требует 3(n - 1) сообщения для входа в критическую секцию и не
является устойчивым к падениям любой машины.

Пример алгоритма может быть продемонстрирован на следующей картинке

TODO(вставить нормальные картинки)

![Mutex](./media/lamutex.gif)

# Ricart–Agrawala algorithm

Этот алгоритм похож на предыдущий, мы тоже будем использовать часы Лэмпорта, но
вместо того, чтобы отсылать подтверждения, а затем release, мы будем отсылать
reply только тогда, когда процесс сам не хочет критической секции и его время
меньше, чем время у запрашиваемого. Если мы поняли, что не хотим отсылать
`Ack`, то мы просто положим к себе в множество, что отошлём как только пройдём
свою критическую секцию (можно сказать, что мы делаем сообщение `Release`, но
по факту это отложенные `Ack`).

Как только запрашиваемый процесс получил все ответы, он входит в критическую
секцию.

Доказательство аналогичное. Один процесс не мог другому нуждающемуся ответить,
так как у одного часы строго меньше.

Заметим, что такой алгоритм уже отсылает 2(n - 1) запросов из-за того, что
полноценно отсутствует `Release` стадия.

![Mutex](./media/ricart.jpg)

# Maekawa algorithm

Этот алгоритм блокировок известен тем, что он требует всего корень из n
запросов, но для этого надо ввести несколько понятий, а именно понятие кворума.

Кворум в общем случае это множество `R_i` для каждого узла `i` со свойствами

* `i` принадлежит `R_i`
* Любые два множества пересекаются на какому-то элементу
* Размер всех множеств не больше K
* Каждый элемент принадлежит не более K множествам

Такие множества существуют размера порядка корень, например, если всех выстроить в квадрат
и для каждого множества взять вертикальную и горизонтальные полосы. Тогда любые
два множества пересекаются как минимум по двум элементам и размер всех две
стороны квадрата.

При желании захватить критическую секцию, узел `i` отсылает всем `R_i` такой
запрос и ждёт от них ответа, входит в критическую секцию и после этого рассылает
всем `R_i` сообщение `Release`.

Каждый узел будет выдавать разрешение на вход в критическую секцию (`Ack`
сообщение) только если никаких разрешений выдано им не было, и все элементы будут
складываться как-то в множество, без времени. Как только такому узлу пришло
сообщение `Release`, он выдает разрешение следующему на вход.

Корректность следует из того, что любые два узла `i` и `j` имеют непустое
пересечение в своих множествах `R_i` и `R_j`, поэтому любой процесс в их
пересечении ответит не более, чем одному из `i` и `j`.

Этот алгоритм может быть устойчив к падениям `m` машин в том случае, если размер
пересечения любых двух множеств не меньше `m + 1`. Например, в схеме с квадратом
мы получаем, что не более одной машины могут упасть и корректность всё ещё
сохранится. Для этого нужен отдельный детектор отказов, чтобы узлы понимали, кто
находится в их множествах `R_i`.

Стоит отметить, что такой алгоритм в чистом виде не является корректным из-за
проблемы deadlock, например, если множества `R_i` и `R_j` пересекаются по 2
элементам, то эти два элемента могут ответить узлам `i` и `j` по-разному по
поводу того, кому можно входить в критическую секцию. Чтобы такого избежать,
можно использовать часы Лэмпорта и делать `RetractVote` запрос, который отбирает
своё подтверждение у запрашиваемого узла.

# Chubby

В компании Google достаточно давно появился свой сервис блокировок &mdash; Chubby.
Такое название пошло из-за одной из старейших британских компаний
[Chubb](https://en.wikipedia.org/wiki/Chubb_Locks), которая занимается выпуском
замков, противоугонных систем. В 1800 годах они сделали замок, который, как
они утверждали, мог задетектить, а пытался ли такой замок быть взломанным и
оставлять на себе пометку. Из-за похожих аналогий так и был сделан сервис
Chubby &mdash; он может сказать, было ли что-то заблокировано или, наоборот,
запрошено на разблокировку надежно и всегда.

Сервис разделен на Chubby Cells, которые хранят в себе 5 узлов, между ними
происходит алгоритм консенсуса [Paxos](https://www.cs.rutgers.edu/~pxk/417/notes/paxos.html),
они умеют договариваться о том, кто захватывает блокировку на тот или иной
"файл":

![Chubby](./media/chubby.jpg)

Для поиска реплик используется DNS, они же затем могут указывать на
текущий мастер. Chubby предоставляет интерфейс, подобный файловой системе, и
узлы могут быть постоянными или временными (привязанными к клиентским сеансам и
автоматически удаляемыми, если они не открыты ни у одного клиента).

Стоит также отметить то, что лок на директорию не распространяется, а сами
файлы выглядят так:

```
/ls/<cell>/a/b/c/d
//^ locking service
```

Это удобнее, чем механизм токенов, так как позволяет выстраивать иерархию
среди многих пользователей, а также следить за квотами, разрешениями и прочим.

Chubby-клиенты могут подписаться на ряд событий, касающихся узлов в файловой
системе, они доставляются после того, как произошло соответствующее изменение.
Таким образом, если клиенту сообщают, что содержимое файла изменилось, он
гарантированно увидит новые данные, если он впоследствии прочитает файл.
Chubby также поддерживает lease механизмы.

Open-source аналог такой функциональности можно считать [Zookeeper](https://zookeeper.apache.org/).
Об алгоритмах консенсуса мы ещё поговорим, а в следующем семестре Вы очень
подробно разберёте алгоритмы консенсуса, в том числе и всю архитектуру Chubby.

# Дальнейшее чтение

* [Chubby at Google](https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf)
* [Chubby from The Morning Paper](https://blog.acolyer.org/2015/02/13/the-chubby-lock-service-for-loosely-coupled-distributed-systems/)
